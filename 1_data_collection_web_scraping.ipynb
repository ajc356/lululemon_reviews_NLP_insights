{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actionable Insights from Lululemon Reviews - Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amanda Cheney  \n",
    "Metis Project 4  \n",
    "Part 1 of 4   \n",
    "November 13, 2020  \n",
    "\n",
    "**Objective**  \n",
    "\n",
    "Natural language processing & unsupervised learning exploration of customer reviews of lululemonâ€™s best-selling sports bras to derive actionable insights for product development and management team and develop a recommender system to provide a curated collection of reviews specifically tailored to customer product needs.\n",
    "\n",
    "**Data Sources**   \n",
    "9,000+ reviews of all 13 of Lululemon's bestselling sports bras, collected using Selenium.  \n",
    "\n",
    "**This Notebook**  \n",
    "Scrapes all reviews for all 13 of Lululemon's bestselling sports bras. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, os\n",
    "\n",
    "from selenium import webdriver\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromedriver = \"/Applications/chromedriver\" # path to the chromedriver executable\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrolled to bottom - moving to product pages.\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome(chromedriver)\n",
    "driver.get('https://shop.lululemon.com/c/women-sports-bras/_/N-7vlZ1z12l0t')\n",
    "# Initiate scrolling so that all products on the page to load\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "time.sleep(5)\n",
    "print(\"Scrolled to bottom - moving to product pages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To scrape: 13 products.\n"
     ]
    }
   ],
   "source": [
    "product_urls = []\n",
    "for i in driver.find_elements_by_xpath('.//h3[@class=\"product-tile__product-name lll-text-body-1\"]//a'):\n",
    "    product_urls.append(i.get_attribute('href'))\n",
    "product_index = 1\n",
    "\n",
    "print('To scrape: {} products.'.format(len(product_urls)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_scraper(product_url):\n",
    "    \"\"\"\n",
    "    Take a product url and extract core product information--product name, list price,\n",
    "    average rating, total number of ratings as well as details for each and every customer \n",
    "    review including: title, content, reviewer name, date of review and store all this information \n",
    "    for each review in a dictionary. Append all dictionaries to a list and return the list. \n",
    "    \"\"\"\n",
    "    product_url = product_url\n",
    "    driver = webdriver.Chrome(chromedriver)\n",
    "    driver.get(product_url)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(15)\n",
    "\n",
    "\n",
    "    my_list=[]\n",
    "    product_dict = {}\n",
    "    product_name = driver.find_element_by_xpath('.//h1[@class=\"pdp-title\"]/div').text.replace(\"\\n\", \" \")\n",
    "    \n",
    "# product information \n",
    "    try:\n",
    "        product_list_price = driver.find_element_by_xpath('.//span[@class=\"price-1SDQy price\"]').text\n",
    "    except Exception as e:\n",
    "        product_list_price = \"\"\n",
    "\n",
    "    try:\n",
    "        product_avg_rating = driver.find_element_by_xpath('.//span[@class=\"bv-rating\"]').text\n",
    "    except Exception as e:\n",
    "        product_avg_rating = \"\"\n",
    "\n",
    "    try:\n",
    "        num_total_ratings = driver.find_element_by_xpath('.//span[@class=\"reviews-link__count\"]').text\n",
    "    except Exception as e:\n",
    "        num_total_ratings = \"\"\n",
    "\n",
    "\n",
    "# load all of the reviews \n",
    "    more_reviews = True \n",
    "\n",
    "    while more_reviews:\n",
    "        try:\n",
    "            button = driver.find_element_by_xpath('.//button[@class=\"bv-content-btn bv-content-btn-pages bv-content-btn-pages-load-more bv-focusable\"]')\n",
    "            driver.execute_script(\"arguments[0].click();\", button)\n",
    "            time.sleep(10)\n",
    "        except NoSuchElementException:\n",
    "            more_reviews = False \n",
    "            print(\"No more reviews to load \")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            reviews=driver.find_elements_by_xpath('//li[@itemprop=\"review\"]')\n",
    "\n",
    "            counter = 0\n",
    "            # loop through reviews to extract review information \n",
    "            for review in reviews:\n",
    "                review_dict = {}\n",
    "                counter+=1\n",
    "                try:\n",
    "                    title = review.find_element_by_xpath('.//div[@class=\"bv-content-title-container\"]').text \n",
    "\n",
    "                except Exception as e:\n",
    "                    title = \"\"\n",
    "\n",
    "                try:\n",
    "                    content = review.find_element_by_xpath('.//div[@class=\"bv-content-summary-body-text\"]').text\n",
    "                except Exception as e:\n",
    "                    content = \"\"    \n",
    "\n",
    "                try:\n",
    "                    rating = review.find_element_by_xpath('.//meta[@itemprop=\"ratingValue\"]').get_attribute(\"content\")\n",
    "                except Exception as e:\n",
    "                    rating = \"\"\n",
    "\n",
    "                try:\n",
    "                    name = review.find_element_by_xpath('.//div[@class=\"bv-author\"]').text\n",
    "                except Exception as e:\n",
    "                    name = \"\"\n",
    "\n",
    "                try:\n",
    "                    date = review.find_element_by_xpath('.//meta[@itemprop=\"datePublished\"]').get_attribute(\"content\")\n",
    "                except Exception as e:\n",
    "                    date = \"\"\n",
    "\n",
    "                review_dict['product_name'] = product_name\n",
    "                review_dict['product_url'] = product_url\n",
    "                review_dict['product_list_price'] = product_list_price\n",
    "                review_dict['product_avg_rating'] = product_avg_rating\n",
    "                review_dict['title'] = title\n",
    "                review_dict['content'] = content\n",
    "                review_dict['rating'] = rating\n",
    "                review_dict['name'] = name\n",
    "                review_dict['date'] = date\n",
    "                review_dict['review counter'] = counter\n",
    "                review_dict['num_total_ratings'] = num_total_ratings\n",
    "\n",
    "                my_list.append(review_dict)\n",
    "            return my_list\n",
    "\n",
    "        # At the end of all reviews scraped for the product\n",
    "        except NoSuchElementException:\n",
    "            print('No more reviews...one product done now!')\n",
    "        driver.close()                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the above code works and will return all 89 reviews for the first product url correctly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more reviews to load \n",
      "89\n"
     ]
    }
   ],
   "source": [
    "zero = my_scraper(product_urls[0])\n",
    "print(len(zero))\n",
    "with open('zero.pickle', 'wb') as to_write:\n",
    "    pickle.dump(zero, to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more reviews to load \n",
      "3036\n"
     ]
    }
   ],
   "source": [
    "one = my_scraper(product_urls[1])\n",
    "print(len(one))\n",
    "with open('one.pickle', 'wb') as to_write:\n",
    "    pickle.dump(one, to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more reviews to load \n",
      "1534\n"
     ]
    }
   ],
   "source": [
    "two = my_scraper(product_urls[2])\n",
    "print(len(two))\n",
    "with open('two.pickle', 'wb') as to_write:\n",
    "    pickle.dump(two, to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more reviews to load \n",
      "53\n"
     ]
    }
   ],
   "source": [
    "three = my_scraper(product_urls[3])\n",
    "print(len(three))\n",
    "with open('three.pickle', 'wb') as to_write:\n",
    "    pickle.dump(three, to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more reviews to load \n",
      "848\n"
     ]
    }
   ],
   "source": [
    "four = my_scraper(product_urls[4])\n",
    "print(len(four))\n",
    "with open('four.pickle', 'wb') as to_write:\n",
    "    pickle.dump(four, to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more reviews to load \n",
      "123\n"
     ]
    }
   ],
   "source": [
    "five = my_scraper(product_urls[5])\n",
    "print(len(five))\n",
    "with open('five.pickle', 'wb') as to_write:\n",
    "    pickle.dump(five, to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more reviews to load \n",
      "691\n"
     ]
    }
   ],
   "source": [
    "six = my_scraper(product_urls[6])\n",
    "print(len(six))\n",
    "with open('six.pickle', 'wb') as to_write:\n",
    "    pickle.dump(six, to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more reviews to load \n",
      "860\n"
     ]
    }
   ],
   "source": [
    "seven = my_scraper(product_urls[7])\n",
    "print(len(seven))\n",
    "with open('seven.pickle', 'wb') as to_write:\n",
    "    pickle.dump(seven, to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more reviews to load \n",
      "308\n"
     ]
    }
   ],
   "source": [
    "eight = my_scraper(product_urls[8])\n",
    "print(len(eight))\n",
    "with open('eight.pickle', 'wb') as to_write:\n",
    "    pickle.dump(eight, to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more reviews to load \n",
      "540\n"
     ]
    }
   ],
   "source": [
    "nine = my_scraper(product_urls[9])\n",
    "print(len(nine))\n",
    "with open('nine.pickle', 'wb') as to_write:\n",
    "    pickle.dump(nine, to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more reviews to load \n",
      "334\n"
     ]
    }
   ],
   "source": [
    "ten = my_scraper(product_urls[10])\n",
    "print(len(ten))\n",
    "with open('ten.pickle', 'wb') as to_write:\n",
    "    pickle.dump(ten, to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more reviews to load \n",
      "225\n"
     ]
    }
   ],
   "source": [
    "eleven = my_scraper(product_urls[11])\n",
    "print(len(eleven))\n",
    "with open('eleven.pickle', 'wb') as to_write:\n",
    "    pickle.dump(eleven, to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more reviews to load \n",
      "486\n"
     ]
    }
   ],
   "source": [
    "twelve = my_scraper(product_urls[12])\n",
    "print(len(twelve))\n",
    "with open('twelve.pickle', 'wb') as to_write:\n",
    "    pickle.dump(twelve, to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
